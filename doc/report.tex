\documentclass[a4paper]{article}

\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}

\newtheorem{algorithm_def}{Algorithm}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

%opening
\title{Robot on icy surface \\ {\small CS6244 Robot Motion Planning \& Control (Prof David Hsu)} }
\author{
  Jahja, Irvan\\
  {\small A0123879R} \\ \texttt{dolphinigle.mailbox@gmail.com}
  \and
  Nguyen, Rang H. M.\\
  {\small A0123879R} \\ \texttt{dolphinigle.mailbox@gmail.com}
}

\begin{document}

\maketitle

\section{Motivation}
% * Provide an intuitive description of the problem domain. Describe the motivation and potential applications.
A car-like robot was dropped on the northern pole few days ago. It has collected various
data on the north pole, and now is ready for pickup at a specific destination,
and with a specific orientation. However, due to global warming, pool of water
forms on the icy surface, and falling into any of them spells certain disfunction
for the robot. Furthermore, icy surface make it very hard for the robot to
follow paths exactly, rendering normal techniques to solve the non-holonomic
motion planning less useful. How are we going to save the robot?

Enter this project. Our algorithm is able to guide the robot to its destination
reliable and, if the robot slips, correct its path, all in real-time. Our algorithm
is able to guide the robot despite uncertainty in its movement -- the preceeding
paragraph gives one such surface that may have unignorable uncertainty.

This project aims to devise such an algorithm, and demonstrates exactly why
we are not allowed to ignore uncertainty in the experiments.

\section{Problem statement}
% * State the technical problem formally. 

\subsection{Configuration space}
We consider a point robot moving on a two dimensional space $\mathbb{R}^2$.
The robot position is denoted by its coordinates $(x, y)$. We restrict the space
to $[0\ldots1, 0\ldots1]$ (that is, we must have $0 \le x \le 1$ and $0 \le y \le 1$).
The robot also has an orientation, denoted by $\theta$. Thus, the robot's
configuration is $(x, y, \theta)$ of dimension $3$.

In addition, there are polygonal obstacles lying on the space. The robot
are not allowed to collide with these obstacles -- doing so results in the
failure of the execution. In addition, the robot is also not allowed to go
past the boundaries of the space. More specifically, whenever the robot's
configuration is within an obstacle or outside the boundaries of the field,
the robot's execution failed.

We assume that the robot is able deduce its current configuration space at
any time -- that is, we assume perfect sensing and knowledge of the configuration space
and the configuration of the robot.

\subsection{Robot movement and non-holonomic constraint}
The robot's movement is governed by a car-like non-holonomic law -- the robot
has a length of $L$ and has a maximum steering distance of $\theta_{\text{max}}$.
The robot can freely adjust its steering distance as long as its absolute
value does not exceed $\theta_{\text{max}}$.  When the
robot's steering angle is at $\alpha$, then the turning radius of the robot
is $L / \sin(\alpha)$, meaning that if the robot steers it with angle of $\alpha$,
it will roughly follow an arc of a circle of radius $L / \sin(\alpha)$.

For simplicity, we assume that the robot do not have the capabilities to move
backwards. Extending the algorithm to support this movement is immediate, but
does not give interesting results.

\subsection{Uncertainty}
In addition, the robot is unable to execute commands perfectly. The coordinates
and orientation of the robot suffers from a noise sampled from an unknown distribution
with known variance (in practice, we can probably measure this variance).
Thus, when we perform integration over the length of the path, by the central
limit theorem, this value is approximately normal. Thus, we model the noise
using normal distribution with variance $\sigma^2$ for the coordinates and
$\sigma_\theta^2$ for the orientation.

\subsection{Objective}
The objective of the robot is to approximately reach a specific location with a
specific orientation. More exactly, the robot is given a goal configuration
$(x_g, y_g, \theta_g)$, and the robot must reach a configuration $(x, y, \theta)$
such that $|\theta - \theta_g| < D$ and $|(x, y)^2 - (x_g, y_g)^2| < T$,
for a threshold $T$ and orientation tolerance $\delta$. Note that the probability
of the robot reaching exactly the configuration $(x_g, y_g, \theta_g)$ might be
zero due to uncertainty in movement.

\section{Algorithm description}
% * Present your algorithm. Describe the input and output if necessary. 
We model the problem using the Markov Decision Process, and use value iteration
to find the optimal actions at each step.
The values of the various parameters associated with the algorithm is
summarized in Figure~\ref{f:notations}.

\subsection{Abstract model}

The markov model of our problem is as follows. Each state in the markov model
would be a configuration in the configuration space. At each state, the robot's
action constitutes of setting the steering angle. Because of the noise, however,
the steering angle does not lead deterministically to the next state.

\subsection{Discretization}
Markov Decision Process assumes a finite number of states and a finite number
of actions. Without discretization, there is an infinite number of
configurations and infinite number of actions. Thus, we discretize the configuration
space and the actions as follows.

\subsubsection{Configuration discretization}
For the space, we discretize each of the axis of the coordinates into $n$
equal-width spaces. We further discretize the angle into $m$ equally-spaced
intervals. Thus, each state is a square cell of $\mathbb{R}^2$ and an orientation
range.

We consider a square cell to be bad if it intersects with an obstacle. Thus, with
very small values of $n$, it might be possible that all square cells are considered
bad, and thus we suggest using values of $n$ that is reasonably high. A rough
guideline is as follows: if the size of the smallest gap is $x$, then we
suggest using $n \ge 2/x$. If all the obstacles are axis aligned, then this
would guarantee that there exists a sequence of adjacent non-bad path from the
start to the goal states if there exists such a path in the non-discretized space.

\subsubsection{Action discretization}
For simplicity, we only consider three possible actions: steer all the way
to the left, steer all the way to the right, or steer straight. That is,
the only three steering angles we consider are $\{-\theta_\text{max}, 0, \theta_\text{max}\}$.

\subsection{Markov model}
The markov decision process (MDP) has the following elements:
\begin{itemize}
  \item S (the set of states): corresponds to each of the discretized cells.
  \item A (the set of actions): $\{-\theta_\text{max}, 0, \theta_\text{max}\}$.
  \item T (the transition function): will be described in section~\ref{sec:t}.
  \item R (the reward function): will be described in section~\ref{sec:r}.
\end{itemize}

In addition, we use an infinite horizon model with discount factor of $\eta$.
We use value iteration to compute the value, stopping when $\epsilon < 0.001$.

\subsubsection{Action configuration transitition}
\label{sec:t}
To model the transition probability function $T(s, a, s')$ (probability of
reaching state $s'$ when executing action $a$ in state $s$), we do the following.
Each action is taken as if it were to be executed until a distance of $\delta$
is travelled. To model the noise, at the projected end of the movement, we
shift the configuration coordinates by a normal distribution with variance
$\sigma^2$ for the coordinates and $\sigma_\theta^2$ for the orientation.

More specifically, $T(s, a, s')$ is computed as follows. Let $(x, y)$ be the
center of the cell of $\mathbb{R}^2$ of state $s$. Consider when the robot
align its steering angle to $a$ until he walked a distance of $\delta$. He
will arrive in another point $(x', y')$, and with a new orientation $\theta'$.
For each $a \in \{x', y', \theta'\}$, consider the normal distribution
with mean equal to $a$ and its corresponding variance. We calculate the probability
that each of these distributions fall under the state $s'$, and multiply all
of them together to obtain $T(s, a, s')$.

\subsubsection{Reward function}
\label{sec:r}
Intuitively, we would like the reward function to deter the robot from entering
obstacles, and reward it if it reaches the goal configuration. More formally,
we give the robot positive reward ($r_g$) if it reaches the goal, and a large negative
reward $r_i$ if it falls to ice. Thus, the reward is actually based on would-be state,
that is, $R(s, a, s')$. Thus, we convert this to $R(s, a)$ by multiplying the
reward by the probability of reaching the state, that is:

\[ R(s, a) = \sum_{s' \in S} T(s, a, s') \cdot R(s, a, s') \]

\subsubsection{Discount factor}

In the general case, the discount factor for an infinite horizon MDP must be
strictly less than $1.0$, because otherwise the value of each state may become
unbounded. However, our model of MDP is special because the reward function is
only ever awarded once, therefore it is impossible for the value of each state
to be unbounded. Because of this, it is possible to set the discount factor to
$1.0$, which is ideal. Indeed we do exactly this in Section~\ref{s:discount}
and show that it gives good result.

With a discount factor of $1$, we can conclude interesting properties from the
value of a state. Let the value of the state be $x$. Then, since the only
rewards available are either from falling to ice or falling to goal, then we
must have

$$x = p_g \cdot r_g + (1 - p_g) \cdot r_i$$

where $p_g$ is the probability of reaching the goal.

Thus, given $x$, we can deduce $p_g$. We can compute this more easily if we
set $r_g = 1$ and $r_i = 0$ (the resulting path will still be the same).
Indeed, since the robot will eventually either reach the goal or failed to
do so, we have:

$$x = p_g \cdot r_g = p_g$$

meaning that the value of each state now has a very direct conclusion of
being exactly the (approximate) probability of reaching the goal!
We discuss the experimental results for this subcase in Section~\ref{s:ideal}.

\section{Results and discussions}
% * Present the theoretical analysis, experiment results, comparison, etc., as applicable.

The values of the parameters used for testing is summarized in Figure~\ref{f:notations}.
We use the value displayed unless stated otherwise.

\begin{figure}
\caption{Various notations we use and its default values for the experiments}
\label{f:notations}
\begin{tabular}{ l | l | l }
  Symbol & Description & Value \\ \hline \hline
  $n$ & Grid resolution & 20 \\ \hline
  $m$ & Orientation resolution & 16 \\ \hline
  $L$ & Length of the car & 0.1 \\ \hline
  $\theta_{\text{max}}$ & Maximum turning angle & $\pi / 4$ \\ \hline
  $\delta$ & Length of a single step of execution & $2^{0.5} / n$ \\ \hline
  $r_g$ & Reward upon reaching the goal & 100 \\ \hline
  $r_i$ & Reward if fall into ice & -10000 \\ \hline
  $\eta$ & Discount factor & 0.95 \\ \hline
  $D$ & Error tolerance on goal coordinates & 0.08 \\ \hline
  $T$ & Error tolerance on goal orientation & 0.4 \\ \hline
  $\sigma^2$ & Axis variance & 0.01 \\ \hline
  $\sigma_\theta^2$ & Orientation variance & 0.1 \\ \hline
  $\epsilon$ & Maximum difference before value iteration stops & 0.001 \\ \hline
\end{tabular}
\end{figure}

The default configuration of obstacles that we use is depicted in
Figure~\ref{f:b_shortest}.

\begin{figure}
\caption{The default configuration used in the subsequent tests. The starting
  configuration is on the bottom left, facing northeast. The goal configuration
  is on the top right, facing east. The configuration consists
  of large corridors and a small corridor that offers shortcut from the start
  and goal configurations. Assuming no noise, the shortest path from the start
to the goal configuration we find is depicted in the figure as well.}
\label{f:b_shortest}
\centerline{\includegraphics[width=5cm]{b_shortest.png}}
\end{figure}

\subsection{On the running time}
On some subsections below, we showed the running time of the algorithm on
a computer with processors 4x Intel$^\text{\textregistered}$ $\text{Core}^{\text{TM}}$ i5-3210M CPU @ 2.50GHz with 4GB RAM. In addition, we do not
include the time required to generate the MDP model, because it is dominated
by the time required to calculate the cumulative density function of the
normal distribution (this is because there is no closed form formula to
calculate this function and thus computation becomes expensive).

\subsection{Figure description}
Figure~\ref{f:b} shows the result of running the algorithm on the default
configuration with default values.

\begin{figure}
\caption{Result of running the algorithm on the default configuration with default values.
Amongst 1000 executions, 780 of them successfully reached the goal configuration.
The value iteration terminated after 150 iterations.}
\label{f:b}
\centerline{\includegraphics[width=5cm]{b.png}}
\end{figure}

In all the figures, including Figure~\ref{f:b}, we adopt the following notations.
The alternating black and red line is the path the robot would follow if
all of its movements are exact. The light grey paths depicts multiple paths
that were generated. In addition, there are 5 other bright colored paths that
are chosen randomly from all the generated paths. The paths may not seem continuous
because we discretize the noise so that it only happens between paths of length
$\delta$.

\subsection{Default value discussion}
In Figure~\ref{f:b}, we make the following observations. The path chosen is
significantly different from the shortest path -- here, the algorithm prefer
to drive the robot to follow the large corridor on the right, even though
the path is longer. This is in line with our intuition that, since the deterrant
of falling into ice is high, it is more beneficial to take the less risky way,
even though it makes the reward somewhat smaller due to applying the discount
factor more.

More interestingly, the appearance of uncertainty makes the movement much more
diverse. There seems to be two major paths: through the center corridor or
through the right corridors. We note that whenever the noise affects the robot
such that it goes "off track", instead of correcting its original track, the
robot may instead follow a different track. This is good as it follows
intuition -- backtracking to the original path may be more dangerous
than following the new path. For example, in the purple path, the robot is
quite affected by the noise that it have to make two different paths: the first
happens on the center intersection, where instead of going straight, it instead
decides to go down since its orientation points too sharply to the south. The
second one happens near the narrow corridor on the left, where its orientation
is affected so much that it points too north, and thus the robot decides to
brace through the narrow corridor instead (and succesfully so).

As an additional observation, we observed that most of the failure occurs on the
right corridor when turning. It seems that it is for this reason that the path
tries to follow the larger corridor on the right, because it requires larger degree
of error before failing, therefore minimizing the chance of failure. However,
an alternative, less pleasant, reasoning of choosing the right corridor might
be that the algorithm tries to make the failure occurs as late as possible,
instead of minimizing the probability of failure -- which is an unfortunate side
effect of this algorithm.

\subsection{Effect of discretization resolution}
The following results seems to suggest that increasing the resolution makes the algorithm
succeed more often.

We ran additional experiments in addition to the one using the default
values. The result is summarized in Figure~\ref{f:resolution}.
The entry in bold is using the default values.
We show four of the results in Figures~\ref{f:b_veryhio} through
Figure~\ref{f:b_lowres_lowio}.
We were unable
to succesfully run the algorithm with a resolution significantly higher than
$20$, due to the computer running out of memory (for storing the MDP model).

\begin{figure}
\caption{Effect of resolutions. Time of execution excludes the time required
         to generate the transition function of the MDP}
\label{f:resolution}
\begin{tabular}{ l | l | l | l }
  $n$ & $m$ & Success rate & Time of execution \\ \hline \hline
  $20$ & $32$ & $0.650 $ & $288$ seconds \\ \hline  % Display this?
  $\mathbf{20}$ & $\mathbf{16}$ & $\mathbf{0.780}$ & $\mathbf{90}$ seconds \\ \hline
  $20$ & $12$ & $0.470$ & $58$ seconds \\ \hline
  $20$ & $8$ & $0.252$ & $29$ seconds \\ \hline  % Display this?

  $15$ & $16$ & $0.188$ & $37$ seconds \\ \hline
  $15$ & $12$ & $0.095$ & $22$ seconds \\ \hline
  $15$ & $8$ & $0.043$ & $14$ seconds \\ \hline

  $10$ & $16$ & $0.059$ & $6$ seconds \\ \hline % Display this?
  $10$ & $12$ & $0.046$ & $4$ seconds \\ \hline
  $10$ & $8$ & $0.040$ & $3.5$ seconds \\ \hline % Display this?
\end{tabular}
\end{figure}

\begin{figure}
\caption{Default configuration with $n=20$, and $m=32$}
\label{f:b_veryhio}
\centerline{\includegraphics[width=5cm]{b_veryhio.png}}
\end{figure}

\begin{figure}
\caption{Default configuration with $n=20$, and $m=8$}
\label{f:b_lowio}
\centerline{\includegraphics[width=5cm]{b_lowio.png}}
\end{figure}

\begin{figure}
\caption{Default configuration with $n=10$, and $m=16$}
\label{f:b_lowres_hio}
\centerline{\includegraphics[width=5cm]{b_lowres_hio.png}}
\end{figure}

\begin{figure}
\caption{Default configuration with $n=10$, and $m=8$}
\label{f:b_lowres_lowio}
\centerline{\includegraphics[width=5cm]{b_lowres_lowio.png}}
\end{figure}

As expected, the higher the resolution, the higher the running time would be.

On performance, in general increasing the resolution increase the success rate of reaching
the goal. However, there is an interesting case when the resolution
for orientation is very high, where the success rate decrease. We observed
the result, and see that for some reason, using very high orientation
resolution yields to a path that is significantly different than when the
resolution is not as high: here, the robot seems to be "bolder" and
likes to pick the narrow passage more often. We hypothesize that the increase
of resolution makes the algorithm's model more accurate and thus makes the model
more confident in taking the shorter path to goal, and hence maximizing the
reward (since taking longer path cause the robot to lose reward by virtue
of the discount factor).

\subsection{Effect of variance}

We vary the variance and the result is summarized in
Figure~\ref{f:variance}. The entry in bold is using the default values.
The individual results are given in Figure~\ref{f:b_veryhivar}, Figure~\ref{f:b_lowvar},
and Figure~\ref{f:b_verylowvar}.

\begin{figure}
\caption{Effect of varying the variance of the noise}
\label{f:variance}
\begin{tabular}{ l | l | l | l }
  $\sigma^2$ & $\sigma_\theta^2$ & Success rate & Time of execution \\ \hline \hline
  $0.02$ & $0.2$ & $0.256$ & $261$ seconds \\ \hline  % Display this?
  $\mathbf{0.01}$ & $\mathbf{0.1}$ & $\mathbf{0.780}$ & $\mathbf{90}$ seconds \\ \hline
  $0.005$ & $0.05$ & $0.939$ & $41$ seconds \\ \hline  % Display this?
  $0.001$ & $0.01$ & $0.995$ & $12$ seconds \\ \hline  % Display this?
\end{tabular}
\end{figure}

\begin{figure}
\caption{Default configuration with $\sigma^2=0.02$, and $\sigma_\theta^2=0.2$}
\label{f:b_veryhivar}
\centerline{\includegraphics[width=5cm]{b_veryhivar.png}}
\end{figure}

\begin{figure}
\caption{Default configuration with $\sigma^2=0.005$, and $\sigma_\theta^2=0.05$}
\label{f:b_lowvar}
\centerline{\includegraphics[width=5cm]{b_lowvar.png}}
\end{figure}

\begin{figure}
\caption{Default configuration with $\sigma^2=0.001$, and $\sigma_\theta^2=0.01$}
\label{f:b_verylowvar}
\centerline{\includegraphics[width=5cm]{b_verylowvar.png}}
\end{figure}


The variance has a direct effect on running time, since higher variance
causes each of the state of MDP to have a higher number of other states
that is reachable with non-negligible probability.

In general, higher variance decreases the performance of the algorithm,
and higher variance makes the robot choose the safer path.

\subsection{Effect of reward}

We vary the proportion of reward upon reaching goal and reward when hitting the
boundaries and summarize the results in
Figure~\ref{f:reward}. The entry in bold is using the default values.
The individual results for the two extreme cases and the middle case are given
in Figure~\ref{f:b_veryhigoal}, Figure~\ref{f:b_eqgoal},
and Figure~\ref{f:b_verylowgoal}.

\begin{figure}
\caption{Effect of varying the rewards}
\label{f:reward}
\begin{tabular}{ l | l | l | l }
  $r_g$ & $r_i$ & Success rate & Time of execution \\ \hline \hline
  $100000$ & $-10$ & $0.559$ & $66$ seconds \\ \hline  % Display this?
  $10000$ & $-100$ & $0.589$ & $66$ seconds \\ \hline  % Display this?
  $1000 $ & $-1000$ & $0.806$ & $69$ seconds \\ \hline  % Display this?
  $\mathbf{100}$ & $\mathbf{-10000}$ & $\mathbf{0.780}$ & $\mathbf{90}$ seconds \\ \hline
  $10$ & $-100000$ & $0.785$ & $99$ seconds \\ \hline  % Display this?
\end{tabular}
\end{figure}

\begin{figure}
\caption{Default configuration with $r_g=100000$, and $r_i=-10$}
\label{f:b_veryhigoal}
\centerline{\includegraphics[width=5cm]{b_veryhigoal.png}}
\end{figure}

\begin{figure}
\caption{Default configuration with $r_g=1000$, and $r_i=-1000$}
\label{f:b_eqgoal}
\centerline{\includegraphics[width=5cm]{b_eqgoal.png}}
\end{figure}

\begin{figure}
\caption{Default configuration with $r_g=10$, and $r_i=-100000$}
\label{f:b_verylowgoal}
\centerline{\includegraphics[width=5cm]{b_verylowgoal.png}}
\end{figure}

There is a trend that awarding penalty for falling greater than reward for
suceeding allows the robot to better find a path. However, we hypothesize that
if we set the discount factor to $1$, then there will be no such trend and all
proportions of rewards will yield the same path.

\subsection{Effect of discount factor}
\label{s:discount}

We vary the discount factor and summarize the results in
Figure~\ref{f:discount}. The entry in bold is using the default values.
The individual results for some entries are given in
Figure~\ref{f:b_zerodiscount} through Figure~\ref{f:b_verylowdiscount}.

\begin{figure}
\caption{Effect of varying the discount factor}
\label{f:discount}
\begin{tabular}{ l | l | l }
  $\eta$ & Success rate & Time of execution \\ \hline \hline
  $1.0$ $0.808$ & $93$ seconds \\ \hline  % Display this?
  $0.999$ $0.818$ & $115$ seconds \\ \hline  % Display this?
  $0.99$ $0.809$ & $113$ seconds \\ \hline  % Display this?
  $\mathbf{0.95}$ & $\mathbf{0.780}$ & $\mathbf{90}$ seconds \\ \hline
  $0.90$ & $0.733$ & $71$ seconds \\ \hline  % Display this?
  $0.50$ & $0.392$ & $32$ seconds \\ \hline  % Display this?
\end{tabular}
\end{figure}

\begin{figure}
\caption{Default configuration with $\eta=1.0$}
\label{f:b_zerodiscount}
\centerline{\includegraphics[width=5cm]{b_zerodiscount.png}}
\end{figure}

\begin{figure}
\caption{Default configuration with $\eta=0.99$}
\centerline{\includegraphics[width=5cm]{b_hidiscount.png}}
\end{figure}

\begin{figure}
\caption{Default configuration with $\eta=0.90$}
\centerline{\includegraphics[width=5cm]{b_lowdiscount.png}}
\end{figure}

\begin{figure}
\caption{Default configuration with $\eta=0.50$}
\label{f:b_verylowdiscount}
\centerline{\includegraphics[width=5cm]{b_verylowdiscount.png}}
\end{figure}

The cases with low discount factor has rather poor performance, since
they are less far sighted and thus their strategy is more of a self-
perseverance rather than goal oriented.

We now highlight the case where the discount factor is $1$ (Figure~\ref{f:zerodiscount}). Compared to the default case (Figure~\ref{f:b}), this path
yields better accuracy. This is confirmed by seeing the results -- in the
case with discount factor $1$, none of the paths created ever attempt to
pass through the narrow corridor on the left, in line with our intuition.
Here, there are no longer incentive for arriving quickly, the only
incentive is to arrive at all. Thus, the robot may make sensible decision
to keep spinning around instead of trying its luck and go through the narrow
passage.

\subsection{Experiments with discount factor of $1$}
\label{s:ideal}

For the following experiments, we use the default values except for the
parameters shown in Figure~\ref{f:idealparam}.

\begin{figure}
  \caption{Non-default variables used in experiments of Section~\ref{s:ideal}}
\label{f:idealparam}
\begin{tabular}{ l | l | l }
  Symbol & Description & Value \\ \hline \hline
  $r_g$ & Reward upon reaching the goal & 1 \\ \hline
  $r_i$ & Reward if fall into ice & 0 \\ \hline
  $\eta$ & Discount factor & 1.0 \\ \hline
  $\epsilon$ & Maximum difference before value iteration stops & 0.000001 \\ \hline
\end{tabular}
\end{figure}

\subsection{U-turn}

In the following scenario, the robot's goal is to return to the original position
with reverse direction. Initially the robot is on the bottom-left of the field
and facing up. He would like to reach the same position, but facing down.
The result of the execution is depicted in Figure~\ref{f:uturn}.

The Markov Decision Process calculation is completed in 160 seconds,
and we found a success rate of 0.931 amongst 1000 trials.

\begin{figure}
\caption{The U-turn obstacle configuration.}
\label{f:uturn}
\centerline{\includegraphics[width=5cm]{uturn.png}}
\end{figure}

\subsection{Narrow passage}

In this scenario, there is a narrow passage in the middle that the robot
must pass through.
The result of the execution is depicted in Figure~\ref{f:narrow}.

The Markov Decision Process calculation is completed in 83 seconds,
and we found a success rate of 0.942 amongst 1000 trials.

\begin{figure}
\caption{The narrow passage obstacle configuration.}
\label{f:narrow}
\centerline{\includegraphics[width=5cm]{narrow.png}}
\end{figure}

\section{Conclusion}
% * Briefly summarize your main findings.
We showed that the Markov Decision Process is able to give reasonable paths
to the robot even when the field is uncertain and constrainted heavily by
non-holonomic constraints. In addition, we showed that under some circumstances,
even when using infinite horizon, we can use a discount factor of $1$ that has
nice implications.

\end{document}


