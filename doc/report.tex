\documentclass[a4paper]{article}

\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}

\newtheorem{algorithm_def}{Algorithm}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

%opening
\title{Robot on icy surface \\ {\small CS6244 Robot Motion Planning \& Control (Prof David Hsu)} }
\author{
  Jahja, Irvan\\
  {\small A0123879R} \\ \texttt{dolphinigle.mailbox@gmail.com}
  \and
  Nguyen, Rang H. M.\\
  {\small A0123879R} \\ \texttt{dolphinigle.mailbox@gmail.com}
}

\begin{document}

\maketitle

\section{Motivation}
% * Provide an intuitive description of the problem domain. Describe the motivation and potential applications.
A car-like robot was dropped on the northern pole few days ago. It has collected various
data on the north pole, and now is ready for pickup at a specific destination,
and with a specific orientation. However, due to global warming, pool of water
forms on the icy surface, and falling into any of them spells certain disfunction
for the robot. Furthermore, icy surface make it very hard for the robot to
follow paths exactly, rendering normal techniques to solve the non-holonomic
motion planning less useful. How are we going to save the robot?

Enter this project. Our algorithm is able to guide the robot to its destination
reliable and, if the robot slips, correct its path, all in real-time. Our algorithm
is able to guide the robot despite uncertainty in its movement -- the preceeding
paragraph gives one such surface that may have unignorable uncertainty.

This project aims to devise such an algorithm, and demonstrates exactly why
we are not allowed to ignore uncertainty in the experiments.

\section{Problem statement}
% * State the technical problem formally. 

\subsection{Configuration space}
We consider a point robot moving on a two dimensional space $\mathbb{R}^2$.
The robot position is denoted by its coordinates $(x, y)$. We restrict the space
to $[0\ldots1, 0\ldots1]$ (that is, we must have $0 \le x \le 1$ and $0 \le y \le 1$).
The robot also has an orientation, denoted by $\theta$. Thus, the robot's
configuration is $(x, y, \theta)$ of dimension $3$.

In addition, there are polygonal obstacles lying on the space. The robot
are not allowed to collide with these obstacles -- doing so results in the
failure of the execution. In addition, the robot is also not allowed to go
past the boundaries of the space. More specifically, whenever the robot's
configuration is within an obstacle or outside the boundaries of the field,
the robot's execution failed.

We assume that the robot is able deduce its current configuration space at
any time -- that is, we assume perfect sensing and knowledge of the configuration space
and the configuration of the robot.

\subsection{Robot movement and non-holonomic constraint}
The robot's movement is governed by a car-like non-holonomic law -- the robot
has a length of $L$ and has a maximum steering distance of $\theta_{\text{max}}$.
The robot can freely adjust its steering distance as long as its absolute
value does not exceed $\theta_{\text{max}}$.  When the
robot's steering angle is at $\alpha$, then the turning radius of the robot
is $L / \sin(\alpha)$, meaning that if the robot steers it with angle of $\alpha$,
it will roughly follow an arc of a circle of radius $L / \sin(\alpha)$.

For simplicity, we assume that the robot do not have the capabilities to move
backwards. Extending the algorithm to support this movement is immediate, but
does not give interesting results.

\subsection{Uncertainty}
In addition, the robot is unable to execute commands perfectly. The coordinates
and orientation of the robot suffers from a noise sampled from an unknown distribution
with known variance (in practice, we can probably measure this variance).
Thus, when we perform integration over the length of the path, by the central
limit theorem, this value is approximately normal. Thus, we model the noise
using normal distribution with variance $\sigma^2$ for the coordinates and
$\sigma_\theta^2$ for the orientation.

\subsection{Objective}
The objective of the robot is to approximately reach a specific location with a
specific orientation. More exactly, the robot is given a goal configuration
$(x_g, y_g, \theta_g)$, and the robot must reach a configuration $(x, y, \theta)$
such that $|\theta - \theta_g| < \delta$ and $|(x, y)^2 - (x_g, y_g)^2| < T$,
for a threshold $T$ and orientation tolerance $\delta$. Note that the probability
of the robot reaching exactly the configuration $(x_g, y_g, \theta_g)$ might be
zero due to uncertainty in movement.

\section{Algorithm description}
% * Present your algorithm. Describe the input and output if necessary. 
We model the problem using the Markov Decision Process, and use value iteration
to find the optimal actions at each step.
The values of the various parameters associated with the algorithm is
summarized in Figure~\ref{f:notations}.

\subsection{Abstract model}

The markov model of our problem is as follows. Each state in the markov model
would be a configuration in the configuration space. At each state, the robot's
action constitutes of setting the steering angle. Because of the noise, however,
the steering angle does not lead deterministically to the next state.

\subsection{Discretization}
Markov Decision Process assumes a finite number of states and a finite number
of actions. Without discretization, there is an infinite number of
configurations and infinite number of actions. Thus, we discretize the configuration
space and the actions as follows.

\subsubsection{Configuration discretization}
For the space, we discretize each of the axis of the coordinates into $n$
equal-width spaces. We further discretize the angle into $m$ equally-spaced
intervals. Thus, each state is a square cell of $\mathbb{R}^2$ and an orientation
range.

\subsubsection{Action discretization}
For simplicity, we only consider three possible actions: steer all the way
to the left, steer all the way to the right, or steer straight. That is,
the only three steering angles we consider are $\{-\theta_\text{max}, 0, \theta_\text{max}\}$.

\subsection{Markov model}
The markov decision process has the following elements:
\begin{itemize}
  \item S (the set of states): corresponds to each of the discretized cells.
  \item A (the set of actions): $\{-\theta_\text{max}, 0, \theta_\text{max}\}$.
  \item T (the transition function): will be described in section~\ref{sec:t}.
  \item R (the reward function): will be described in section~\ref{sec:r}.
\end{itemize}

In addition, we use an infinite horizon model with discount factor of $\eta$.

\subsubsection{Action configuration transitition}
\label{sec:t}
To model the transition probability function $T(s, a, s')$ (probability of
reaching state $s'$ when executing action $a$ in state $s$), we do the following.
Each action is taken as if it were to be executed until a distance of $\delta$
is travelled. To model the noise, at the projected end of the movement, we
shift the configuration coordinates by a normal distribution with variance
$\sigma^2$ for the coordinates and $\sigma_\theta^2$ for the orientation.

More specifically, $T(s, a, s')$ is computed as follows. Let $(x, y)$ be the
center of the cell of $\mathbb{R}^2$ of state $s$. Consider when the robot
align its steering angle to $a$ until he walked a distance of $\delta$. He
will arrive in another point $(x', y')$, and with a new orientation $\theta'$.
For each $a \in \{x', y', \theta'\}$, consider the normal distribution
with mean equal to $a$ and its corresponding variance. We calculate the probability
that each of these distributions fall under the state $s'$, and multiply all
of them together to obtain $T(s, a, s')$.

\subsubsection{Reward function}
\label{sec:r}
We give the robot positive reward ($r_g$) if it reaches the goal, and a large negative
reward $r_i$ if it falls to ice. Thus, the reward is actually based on would-be state,
that is, $R(s, a, s')$. Thus, we convert this to $R(s, a)$ by multiplying the
reward by the probability of reaching the state, that is:

\[ R(s, a) = \sum_{s' \in S} T(s, a, s') \cdot R(s, a, s') \]

\section{Results and discussions}
% * Present the theoretical analysis, experiment results, comparison, etc., as applicable.

The values of the parameters used for testing is summarized in Figure~\ref{f:notations}.
We use the value displayed unless stated otherwise.

\begin{figure}
\label{f:notations}
\caption{Various notations we use and its default values for the experiments}
\begin{tabular}{ l | l | l }
  Symbol & Description & Value \\ \hline \hline
  $n$ & Grid resolution & 20 \\ \hline
  $m$ & Orientation resolution & 16 \\ \hline
  $L$ & Length of the car & 0.1 \\ \hline
  $\theta_{\text{max}}$ & Maximum turning angle & $\pi / 4$ \\ \hline
  $\delta$ & Length of a single step of execution & $2^{0.5} / n$ \\ \hline
  $r_g$ & Reward upon reaching the goal & 100 \\ \hline
  $r_i$ & Reward if fall into ice & -10000 \\ \hline
  $\eta$ & Discount factor & 0.95 \\ \hline
\end{tabular}
\end{figure}

\subsection{Effect of axis resolution}

\subsection{Effect of variance}

\subsection{Effect of reward}

\subsection{Effect of discount factor}

\section{Conclusion}
% * Briefly summarize your main findings.
We showed that the Markov Decision Process is able to give reasonable paths
to the robot even when the field is uncertain and constrainted heavily by
non-holonomic constraints.

\end{document}

